import time
import requests
import logging
import hashlib
import numpy as np
from typing import Dict
from datetime import datetime, timedelta
import json

CONFIG_get_image = {
    'appCode': 'JC_PIDLB',
    'token': '9b78f9ab773774f5b2c4b627ff007152',
    'api_url': 'http://deeplog-ck-robot.jd.com/rest/api/convertDataIntoImages',
}

#é‰´æƒ
def get_auth_headers_get_image() -> dict:
    """ç”Ÿæˆé‰´æƒè¯·æ±‚å¤´"""
    now = datetime.now()
    time_str = now.strftime("%H%M%Y%m%d")
    timestamp = str(int(time.time()))
    sign = hashlib.md5(f"#{CONFIG_get_image['token']}NP{time_str}".encode()).hexdigest()
    return {
        "Content-Type": "application/json",
        "appCode": CONFIG_get_image['appCode'],
        "sign": sign,
        "time": timestamp,
    }

def generate_timeseries_chart_url(
    data: Dict[str, Dict[str, float]],
    chart_type: str = "svg" ,
    metrics_name: str = None,
    base_line:float = None
) -> str:
    """
    ç”Ÿæˆæ—¶åºå›¾å¹¶è¿”å›åœ¨çº¿é¢„è§ˆé“¾æ¥
    Args:
        data: æ—¶åºæ•°æ® {æŒ‡æ ‡å: {æ—¶é—´ç‚¹: æ•°å€¼}}     
    Returns:
        åœ¨çº¿é¢„è§ˆé“¾æ¥
    """
    # æ ¹æ®chart_typeè®¾ç½®filename
    if chart_type == "svg":
        filename = f"{metrics_name}.svg"
    # elif chart_type == "png":
    #     filename = "chart.png"
    # else:
    #     filename = "chart.png"  # é»˜è®¤
    
    params = {
        "timeSeriesData": data,
        "filename": filename,  # åŒ…å«åç¼€çš„æ–‡ä»¶å
        "title": metrics_name,
        "width":1500,
        "height":700,
        "ossType" : 1,
        "showLegend":True,
        "usingBaseLine":True,
        "baseLine":base_line,
        "baseLineName":"é˜ˆå€¼"
    }
    resp = requests.post(
        CONFIG_get_image['api_url'],
        headers=get_auth_headers_get_image(),
        json=params,
        timeout=300
    )
    result = resp.json()
    
    # print("ç”Ÿæˆå›¾ç‰‡çš„ä¿¡æ¯ï¼š\n")
    # print(result)
    if result.get("code") == 200:
        return result["data"]["src"]
    else:
        raise Exception(f"å›¾è¡¨ç”Ÿæˆå¤±è´¥: {result.get('msg', 'æœªçŸ¥é”™è¯¯')}")

def get_yesterday_time(cluster_name, start_time, end_time):
    def shift(time_str):
        dt = datetime.strptime(time_str, "%Y-%m-%d %H:%M:%S")
        return (dt - timedelta(days=1)).strftime("%Y-%m-%d %H:%M:%S")
    return cluster_name, shift(start_time), shift(end_time)


def get_previous_30_minutes(device_id, time1, time2):
    def shift(time_str):
        dt = datetime.strptime(time_str, "%Y-%m-%d %H:%M:%S")
        return (dt - timedelta(minutes=30)).strftime("%Y-%m-%d %H:%M:%S")
    
    return device_id, shift(time1), shift(time2)
# --- é‰´æƒä¸è¯·æ±‚æ¨¡å— ---
 
def npa_summary_data(postdata, apiurl, method="POST"):
    user = "xiehanqi.jackson"
    ctime = str(int(time.time()))
    new_key = f"{user}|{ctime}"
    # è®¡ç®—ç­¾å
    api_header_val = f"{hashlib.md5(new_key.encode()).hexdigest()}|{ctime}"
    url = f'http://npa-test.jd.com{apiurl}'
    user_agent = 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)'
    headers = {'auth-api': api_header_val, 'auth-user': user, 'Content-Type': "application/json", 'User-Agent': user_agent}
    try:
        if method == "POST":
            response = requests.post(url, json=postdata, headers=headers)
        elif method == "GET":
            response = requests.get(url, params=postdata, headers=headers)
        else:
            return {}
        response.raise_for_status()
        logging.info(f"code:{response.status_code}, response:{response.text}")
        return response.json()
    except requests.RequestException as e:
        logging.error(f"API request error: {e}")
        return {}
 
def get_cluster_tp_api(groupname,begin_time,end_time):
    postdata = {
        "groupname": groupname,
        "begin_time": begin_time,
        "end_time": end_time
    }
    apiurl = "/prod-api/api/v2/analysis/deeplog/querytpn?format=json"
    result = npa_summary_data(postdata, apiurl)
    # --- è¡¥å……é”™è¯¯å¤„ç† ---
    # 1. æ£€æŸ¥åŸºç¡€è¯·æ±‚æ˜¯å¦æˆåŠŸ (npa_summary_data å¯èƒ½è¿”å› None)
    if result is None:
        logging.error("Failed to fetch data due to request exception.")
        return {"status": "error", "message": "Request failed", "code": -1}
 
    # 2. æ£€æŸ¥ä¸šåŠ¡çŠ¶æ€ç  (å‡è®¾ API è¿”å›æ ¼å¼ä¸­åŒ…å« code å­—æ®µ)
    # å¾ˆå¤š API è¿”å›æ ¼å¼ä¸º {"code": 200, "data": ...} æˆ– {"code": 500, "msg": "error"}
    if "code" in result:
        business_code = result.get("code")
        # å‡è®¾ 200 è¡¨ç¤ºæˆåŠŸï¼Œå…¶ä»–å‡ä¸ºä¸šåŠ¡é”™è¯¯
        if business_code != 200: 
            error_msg = result.get("message", result.get("msg", "Unknown business error"))
            logging.error(f"API business error: code={business_code}, message={error_msg}")
            return {
                "status": "error", 
                "message": error_msg, 
                "code": business_code,
                "raw_response": result
            }
 
    # 3. æ£€æŸ¥æ•°æ®å†…å®¹æ˜¯å¦å­˜åœ¨ (é˜²æ­¢ code=200 ä½† data ä¸ºç©º)
    # å‡è®¾æœ‰æ•ˆæ•°æ®åœ¨ 'data' å­—æ®µä¸­
    # if "data" not in result or not result["data"]:
    #     logging.warning("API response successful but no data found.")
    #     return {"status": "empty", "message": "No data available", "code": 0}
 
    # å…¨éƒ¨æ£€æŸ¥é€šè¿‡ï¼Œè¿”å›å®Œæ•´ç»“æœ
    return result
 
# --- æ•°æ®å¤„ç†æ¨¡å— ---
 
def extract_values(response):
    """
    è®¡ç®—æ¥å£è¿”å›æ•°æ®ä¸­ connect_delay = total_delay_tp - srv_delay_tp ã€‚
    """
    diff_series = []
    
    # ç›´æ¥è·å–æ•°æ®ï¼Œä¸å†åˆ¤æ–­ code==200 å’Œ data æ˜¯å¦å­˜åœ¨
    data_content = response.get('data', {})
    series_data = data_content.get('series_data', [])
    
    srv_delay_vals = []
    total_delay_vals = []
    
    # æå–ç‰¹å®šåºåˆ—çš„æ•°æ®
    if isinstance(series_data, list):
        for item in series_data:
            if item.get('name') == 'srv_delay_tp':
                srv_delay_vals = item.get('value', [])
            elif item.get('name') == 'total_delay_tp':
                total_delay_vals = item.get('value', [])
    
    # è®¡ç®—å·®å€¼ (total_delay_tp - srv_delay_tp)
    if srv_delay_vals and total_delay_vals:
        min_len = min(len(srv_delay_vals), len(total_delay_vals))
        for i in range(min_len):
            diff_series.append(total_delay_vals[i] - srv_delay_vals[i])
            
    return diff_series
 
def analyze_data(A, B, C,x):
    """
    è®¡ç®—æ•°æ®çš„æ—¥ç¯æ¯”ã€çŸ­æœŸç¯æ¯”ã€å‡å€¼å¹¶åˆ¤æ–­æ˜¯å¦è¶…è¿‡é˜ˆå€¼ã€‚
    ç¯æ¯”ç»“æœæ ¼å¼åŒ–ä¸ºç®­å¤´+ç¬¦å·+ç™¾åˆ†æ¯”çš„å­—ç¬¦ä¸²ã€‚
    ä¸Šæ¶¨ï¼šâ†‘+xx.xx%
    ä¸‹é™ï¼šâ†“-xx.xx%
    
    å‚æ•°:
    A (list): å½“å‰æ—¶æ®µæ•°æ®åˆ—è¡¨
    B (list): ä¸Šä¸€ä¸ªå‘¨æœŸæ•°æ®åˆ—è¡¨ (ç”¨äºæ—¥ç¯æ¯”)
    C (list): çŸ­æœŸå†å²æ•°æ®åˆ—è¡¨ (ç”¨äºçŸ­æœŸç¯æ¯”)
    x (int): é˜ˆå€¼
    
    è¿”å›:
    dict: åŒ…å«è®¡ç®—ç»“æœçš„å­—å…¸
    """
    
    # è®¡ç®—å‡å€¼ï¼Œå¤„ç†åˆ—è¡¨ä¸ºç©ºçš„æƒ…å†µä»¥é˜²é™¤é›¶é”™è¯¯
    avg_A = sum(A) / len(A) if A else 0
    avg_B = sum(B) / len(B) if B else 0
    avg_C = sum(C) / len(C) if C else 0
    
    
    # è®¡ç®—æ—¥ç¯æ¯”æ•°å€¼
    try:
        day_ratio_val = (avg_A - avg_B) / avg_B
    except ZeroDivisionError:
        day_ratio_val = 0
 
    # è®¡ç®—çŸ­æœŸç¯æ¯”æ•°å€¼
    try:
        short_ratio_val = (avg_A - avg_C) / avg_C
    except ZeroDivisionError:
        short_ratio_val = 0
 
    # æ ¼å¼åŒ–é€»è¾‘ï¼šä¸Šæ¶¨ç”¨â†‘å’Œ+ï¼Œä¸‹é™ç”¨â†“å’Œ-
    # è¿™é‡Œçš„ç¬¦å·ä¸æ•°å€¼æœ¬èº«çš„æ­£è´Ÿå·ä¸€è‡´
    
    # æ—¥ç¯æ¯”å­—ç¬¦ä¸²ç”Ÿæˆ
    if day_ratio_val >= 0:
        day_ratio_str = f"â†‘{day_ratio_val * 100:+.2f}%"
    else:
        day_ratio_str = f"â†“{day_ratio_val * 100:+.2f}%"
 
    # çŸ­æœŸç¯æ¯”å­—ç¬¦ä¸²ç”Ÿæˆ
    if short_ratio_val >= 0:
        short_ratio_str = f"â†‘{short_ratio_val * 100:+.2f}%"
    else:
        short_ratio_str = f"â†“{short_ratio_val * 100:+.2f}%"
 
    # å‡å€¼
    mean_val = int(avg_A)
    # print(f"å½“å‰æ—¶æ®µçš„å‡å€¼: {mean_val}")
    # print(f"çŸ­æœŸå‡å€¼: {avg_C}")
 
    # åˆå§‹åŒ–å‘Šè­¦ä¿¡æ¯
    alert_messages = []
    
    # åˆ¤æ–­æ—¥ç¯æ¯”æ˜¯å¦ > 30%
    if day_ratio_val * 100 > 30:
        alert_messages.append(f"æ—¥ç¯æ¯”>30%")
        
    # åˆ¤æ–­çŸ­æœŸç¯æ¯”æ˜¯å¦ > 30%
    if short_ratio_val * 100 > 30:
        alert_messages.append(f"çŸ­æœŸç¯æ¯”>30%")
    
    # åˆ¤æ–­æœ€å¤§å€¼æ˜¯å¦è¶…è¿‡å›ºå®šé˜ˆå€¼
    max_A = max(A) if A else 0
    if max_A >= x:
        alert_messages.append(f"Max = {max_A} >{x} (é˜ˆå€¼)")
        print(max_A)
    
    # ç»„åˆæœ€ç»ˆå‘Šè­¦æ–‡æœ¬
    if alert_messages:
        threshold_status = "å¼‚å¸¸ ," + ",".join(alert_messages)
    else:
        threshold_status = "æ­£å¸¸"
    
    #----------------------TP_90ã€TP_95ã€å³°å€¼ æ·»åŠ 
    # æ’åºæ•°æ®
    sorted_data = sorted([float(x) for x in A])
    n = len(sorted_data)
    
    # å³°å€¼ï¼ˆæœ€å¤§å€¼ï¼‰
    peak = max(sorted_data)
    
    # P90ï¼ˆç¬¬90ç™¾åˆ†ä½ï¼‰
    idx_90 = int(0.9 * n)
    p90 = sorted_data[idx_90] if idx_90 < n else sorted_data[-1]
    
    # P95ï¼ˆç¬¬95ç™¾åˆ†ä½ï¼‰
    idx_95 = int(0.95 * n)
    p95 = sorted_data[idx_95] if idx_95 < n else sorted_data[-1]
    return {
        "æ—¥ç¯æ¯”": day_ratio_str,
        "çŸ­æœŸç¯æ¯”": short_ratio_str,
        "å‡å€¼": mean_val,
        "å³°å€¼":peak,
        "TP_90":p90,
        "TP_95":p95,
        "æ˜¯å¦å‘Šè­¦": threshold_status
    }
def extract_connect_delay_timeSeriesData(api_response):
    """
    è½¬æ¢APIå“åº”æ•°æ®ä¸ºæŒ‡å®šæ ¼å¼
    
    Args:
        api_response: åŸå§‹çš„APIå“åº”å­—å…¸
    
    Returns:
        dict: è½¬æ¢åçš„æ ¼å¼
    """
    try:
        # è§£æJSONå­—ç¬¦ä¸²ï¼ˆå¦‚æœä¼ å…¥çš„æ˜¯å­—ç¬¦ä¸²ï¼‰
        if isinstance(api_response, str):
            data = json.loads(api_response)
        else:
            data = api_response
        
        if data.get('code') != 200:
            raise ValueError(f"APIè¿”å›é”™è¯¯: {data.get('message', 'Unknown error')}")
        
        chart_data = data['data']
        x_data = chart_data['x_data']
        
        # æŸ¥æ‰¾ç³»åˆ—æ•°æ®
        series = {}
        for s in chart_data['series_data']:
            series[s['name']] = s['value']
        
        srv_delay = series.get('srv_delay_tp', [])
        total_delay = series.get('total_delay_tp', [])
        
        if len(x_data) != len(srv_delay) or len(x_data) != len(total_delay):
            raise ValueError("xè½´æ•°æ®ä¸yè½´æ•°æ®é•¿åº¦ä¸åŒ¹é…")
        
        # è®¡ç®—connect_delay
        result = {"connect_delay": {}}
        for timestamp, srv_val, total_val in zip(x_data, srv_delay, total_delay):
            connect_delay = total_val - srv_val
            result["connect_delay"][timestamp] = round(connect_delay, 1)
        
        return result
        
    except (KeyError, ValueError, json.JSONDecodeError) as e:
        raise ValueError(f"æ•°æ®æ ¼å¼é”™è¯¯: {str(e)}")
def day_add_1(data):
    """
    å°†å­—å…¸ä¸­æ‰€æœ‰æ—¶é—´é”®æ•´ä½“å‘åæ¨ç§»1å¤©
    """
    result = {}
    for metric, time_dict in data.items():
        new_time_dict = {}
        for time_str, value in time_dict.items():
            # å°†å­—ç¬¦ä¸²æ—¶é—´è½¬æ¢ä¸ºdatetimeï¼ŒåŠ ä¸Š1å¤©ï¼Œå†è½¬å›å­—ç¬¦ä¸²
            dt = datetime.strptime(time_str, "%Y-%m-%d %H:%M:%S")
            new_dt = dt + timedelta(days=1)
            new_time_str = new_dt.strftime("%Y-%m-%d %H:%M:%S")
            new_time_dict[new_time_str] = value
        
        result[metric] = new_time_dict
    
    return result
def day_add_30(data, minutes=30):
    """
    å°†å­—å…¸ä¸­æ‰€æœ‰æ—¶é—´é”®æ•´ä½“å‘åæ¨ç§»æŒ‡å®šåˆ†é’Ÿæ•°
    
    Args:
        data: åŒ…å«æ—¶é—´æˆ³ä½œä¸ºé”®çš„åµŒå¥—å­—å…¸
        minutes: è¦æ¨ç§»çš„åˆ†é’Ÿæ•°ï¼ˆé»˜è®¤30åˆ†é’Ÿï¼‰
        
    Returns:
        æ›´æ–°åçš„å­—å…¸
    """
    return {
        metric: {
            (datetime.strptime(t, "%Y-%m-%d %H:%M:%S") + timedelta(minutes=minutes))
            .strftime("%Y-%m-%d %H:%M:%S"): v
            for t, v in time_dict.items()
        }
        for metric, time_dict in data.items()
    }
    
def merge_dicts(dict1, dict2, dict3):
    """
    åˆå¹¶ä¸¤ä¸ªç›‘æ§æŒ‡æ ‡å­—å…¸
    å¦‚æœé”®ç›¸åŒï¼Œç¬¬äºŒä¸ªå­—å…¸çš„å€¼ä¼šè¦†ç›–ç¬¬ä¸€ä¸ªå­—å…¸çš„å€¼
    """
    return {**dict1, **dict2, **dict3}


# --- ç»¼åˆåˆ†ææ¨¡å— ---
    # name="get_connect_delay_analysis",
    # description="è·å–é›†ç¾¤å»ºè¿è€—æ—¶(TP)åˆ†æç»“æœ",
    # args_schema={
    #     "type": "object",
    #     "properties": {
    #         "groupname": {"type": "string", "description": "é›†ç¾¤åç§°(ä¾‹å¦‚: lf-lan-ha1)"},
    #         "begin_time": {
    #             "type": "string",
    #             "description": "å¼€å§‹æ—¶é—´(ä¾‹å¦‚: 2026-01-14 11:00:00)",
    #         },
    #         "end_time": {
    #             "type": "string",
    #             "description": "ç»“æŸæ—¶é—´(ä¾‹å¦‚: 2026-01-14 11:30:00)",
    #         },
    #     },
    #     "required": ["groupname", "begin_time", "end_time"],
    # }
def get_cluster_connect_delay_analysis(groupname, begin_time, end_time):
    """
    è·å–ä¸‰ä¸ªæ—¶é—´æ®µçš„é›†ç¾¤QPSæ•°æ®
    """
    # è·å–æ˜¨å¤©æ—¶é—´çš„å‚æ•°
    B_groupname, B_begin_time, B_end_time = get_yesterday_time(groupname, begin_time, end_time)
    # è·å–å‰30åˆ†é’Ÿçš„å‚æ•°
    C_groupname, C_begin_time, C_end_time = get_previous_30_minutes(groupname, begin_time, end_time)
    
    # è°ƒç”¨APIå‡½æ•°è·å–åŸå§‹æ•°æ®
    A_raw = get_cluster_tp_api(groupname, begin_time, end_time)
    B_raw = get_cluster_tp_api(B_groupname, B_begin_time, B_end_time)
    C_raw = get_cluster_tp_api(C_groupname, C_begin_time, C_end_time)
    
    
    A_ts = extract_connect_delay_timeSeriesData(A_raw)
    B_ts = extract_connect_delay_timeSeriesData(B_raw)
    C_ts = extract_connect_delay_timeSeriesData(C_raw)
    
    if "connect_delay" in A_ts:
        A_ts["ğŸ“Š å½“å‰è¶‹åŠ¿"] = A_ts.pop("connect_delay")
        
    if "connect_delay" in B_ts:
        B_ts["ğŸ“ˆ æ—¥ç¯æ¯”"] = B_ts.pop("connect_delay")
        
    if "connect_delay" in C_ts:
        C_ts["ğŸ“ˆ 30åˆ†é’Ÿç¯æ¯”"] = C_ts.pop("connect_delay")
    
    merge_data = merge_dicts(A_ts, day_add_1(B_ts), day_add_30(C_ts))
    
    image_url = generate_timeseries_chart_url(merge_data,"svg","LBå»ºè¿è€—æ—¶",20)

    # ä»APIè¿”å›æ•°æ®ä¸­æå–QPSå€¼åˆ—è¡¨
    A_tp_data = extract_values(A_raw)
    B_tp_data = extract_values(B_raw)
    C_tp_data = extract_values(C_raw)
    
    
    obj = {
        "info":analyze_data(A_tp_data, B_tp_data, C_tp_data,20),
        "image_url":image_url
    }
    return obj

# r = get_cluster_connect_delay_analysis("lf-lan-ha1","2026-02-05 10:00:00","2026-02-05 10:30:00")
# print(r)
