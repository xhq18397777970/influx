import time
import requests
import logging
import hashlib
import json
from typing import Dict, List, Any, Union
from datetime import datetime, timedelta
#qpsï¼šæè¿°æœåŠ¡å™¨å•ä½æ—¶é—´å†…å¤„ç†çš„è¯·æ±‚æ•°é‡ã€‚è¡¡é‡æœåŠ¡å™¨çš„å¤„ç†è¯·æ±‚èƒ½åŠ›
#bps: æè¿°ç½‘ç»œä¼ è¾“é€Ÿåº¦ï¼Œå³æ¯ç§’ä¼ è¾“çš„æ•°æ®é‡å¤§å°ã€‚è¡¡é‡ç½‘ç»œå¸¦å®½å’Œååé‡




# é…ç½®å‚æ•°
CONFIG = {
    'appCode': 'JC_PIDLB',
    'token': '9b78f9ab773774f5b2c4b627ff007152',
    'api_url': 'http://deeplog-lb-api.jd.com/',
}

CONFIG_get_image = {
    'appCode': 'JC_PIDLB',
    'token': '9b78f9ab773774f5b2c4b627ff007152',
    'api_url': 'http://deeplog-ck-robot.jd.com/rest/api/convertDataIntoImages',
}

#é‰´æƒ
def get_auth_headers_get_image() -> dict:
    """ç”Ÿæˆé‰´æƒè¯·æ±‚å¤´"""
    now = datetime.now()
    time_str = now.strftime("%H%M%Y%m%d")
    timestamp = str(int(time.time()))
    sign = hashlib.md5(f"#{CONFIG_get_image['token']}NP{time_str}".encode()).hexdigest()
    return {
        "Content-Type": "application/json",
        "appCode": CONFIG_get_image['appCode'],
        "sign": sign,
        "time": timestamp,
    }

def generate_timeseries_chart_url(
    data: Dict[str, Dict[str, float]],
    chart_type: str = "svg" ,
    metrics_name: str = None,
    base_line:float = None
) -> str:
    """
    ç”Ÿæˆæ—¶åºå›¾å¹¶è¿”å›åœ¨çº¿é¢„è§ˆé“¾æ¥
    Args:
        data: æ—¶åºæ•°æ® {æŒ‡æ ‡å: {æ—¶é—´ç‚¹: æ•°å€¼}}     
    Returns:
        åœ¨çº¿é¢„è§ˆé“¾æ¥
    """
    # æ ¹æ®chart_typeè®¾ç½®filename
    if chart_type == "svg":
        filename = f"{metrics_name}.svg"
    # elif chart_type == "png":
    #     filename = "chart.png"
    # else:
    #     filename = "chart.png"  # é»˜è®¤
    
    params = {
        "timeSeriesData": data,
        "filename": filename,  # åŒ…å«åç¼€çš„æ–‡ä»¶å
        "title": metrics_name,
        "width":1500,
        "height":700,
        "ossType" : 1,
        "showLegend":True,
        "usingBaseLine":True,
        "baseLine":base_line,
        "baseLineName":"é˜ˆå€¼"
    }
    resp = requests.post(
        CONFIG_get_image['api_url'],
        headers=get_auth_headers_get_image(),
        json=params,
        timeout=300
    )
    result = resp.json()
    
    # print("ç”Ÿæˆå›¾ç‰‡çš„ä¿¡æ¯ï¼š\n")
    # print(result)
    if result.get("code") == 200:
        return result["data"]["src"]
    else:
        raise Exception(f"å›¾è¡¨ç”Ÿæˆå¤±è´¥: {result.get('msg', 'æœªçŸ¥é”™è¯¯')}")


    
def get_yesterday_time(cluster_name, start_time, end_time):
    def shift(time_str):
        dt = datetime.strptime(time_str, "%Y-%m-%d %H:%M:%S")
        return (dt - timedelta(days=1)).strftime("%Y-%m-%d %H:%M:%S")

    return cluster_name, shift(start_time), shift(end_time)


def get_previous_30_minutes(device_id, time1, time2):
    def shift(time_str):
        dt = datetime.strptime(time_str, "%Y-%m-%d %H:%M:%S")
        return (dt - timedelta(minutes=30)).strftime("%Y-%m-%d %H:%M:%S")
    
    return device_id, shift(time1), shift(time2)

def datetime_str_to_timestamp(dt_str: str) -> int:
    """å°†æ—¶é—´å­—ç¬¦ä¸²è½¬æ¢ä¸ºæ¯«ç§’æ—¶é—´æˆ³"""
    try:
        dt = datetime.strptime(dt_str, '%Y-%m-%d %H:%M:%S')
        return int(dt.timestamp() * 1000)
    except ValueError:
        # å°è¯•å®¹é”™å¤„ç†ï¼Œå¦‚æœåªæœ‰æ—¥æœŸ
        try:
            dt = datetime.strptime(dt_str, '%Y-%m-%d')
            return int(dt.timestamp() * 1000)
        except ValueError:
            raise ValueError(f"æ—¶é—´æ ¼å¼é”™è¯¯: {dt_str}ï¼Œåº”ä¸º 'YYYY-MM-DD HH:MM:SS'")
 
def get_np_auth_headers(app_code: str, token: str) -> dict:
    now = datetime.now()
    # ä¿®æ­£æ—¶é—´æ ¼å¼:%H%M%Y%m%d (å°æ—¶åˆ†é’Ÿå¹´æœˆæ—¥)
    time_str = now.strftime("%H%M%Y%m%d")
    timestamp = str(int(time.time() * 1000))
    # ç­¾åå­—ç¬¦ä¸²
    sign_str = f"#{token}NP{time_str}"
    sign = hashlib.md5(sign_str.encode('utf-8')).hexdigest()
 
    headers = {
        "Content-Type": "application/json;charset=utf-8",  
        "appCode": app_code,
        "sign": sign,
        "time": timestamp,
    }
    return headers
 
def get_cluster_qps_api(cluster_name: str, start_time: str, end_time: str) -> dict:
    """
    è·å–é›†ç¾¤æµé‡QPSä¿¡æ¯ (åº•å±‚æ¥å£è°ƒç”¨)
    
    Args:
        cluster_name: é›†ç¾¤åç§°
        start_time: å¼€å§‹æ—¶é—´å­—ç¬¦ä¸² (YYYY-MM-DD HH:MM:SS)
        end_time: ç»“æŸæ—¶é—´å­—ç¬¦ä¸² (YYYY-MM-DD HH:MM:SS)
        
    Returns:
        dict: åŒ…å«QPSæ•°æ®çš„å­—å…¸æˆ–é”™è¯¯ä¿¡æ¯
    """
    # è½¬æ¢æ—¶é—´æ ¼å¼
    try:
        start_ts = datetime_str_to_timestamp(start_time)
        end_ts = datetime_str_to_timestamp(end_time)
    except ValueError as e:
        return {"code": -1, "message": str(e)}
 
    headers = get_np_auth_headers(CONFIG['appCode'], CONFIG['token'])
    url = f"{CONFIG['api_url']}v1/search"
    
    params = {
        "size": 10,  # è¿™é‡Œçš„sizeå¯èƒ½éœ€è¦æ³¨æ„ï¼Œå¦‚æœæ˜¯å¤§é‡æ•°æ®ç‚¹å¯èƒ½éœ€è¦è°ƒæ•´æˆ–åˆ†é¡µ
        "bizName": "lbha",
        "resource": "count",
        "timeRange": {
            "start": start_ts,
            "end": end_ts
        },
        "interval": "10s", 
        "match": [{
            "eq": {
                "lb-node-name": [cluster_name]
            } 
        }],
        "algorithm": {
            "algorithmName": "sum",
        }
    }
    
    try:
        response = requests.post(url, headers=headers, json=params, timeout=30)
        response.raise_for_status()
        raw_data = response.json()
        return raw_data
        
    except requests.exceptions.RequestException as e:
        error_info = {
            "code": -1,
            "message": f"è¯·æ±‚å¤±è´¥: {str(e)}",
            "error_type": type(e).__name__
        }
        if hasattr(e, 'response') and e.response is not None:
            error_info["response_text"] = e.response.text
            error_info["status_code"] = e.response.status_code
        return error_info
# --- æ•°æ®æå–ä¸è½¬æ¢ ---

def extract_values(data):
    """
    ä»æ¥å£è¿”å›çš„æ•°æ®ä¸­æå–æ‰€æœ‰å€¼æ„æˆåˆ—è¡¨
    """
    # ç¡®ä¿æ•°æ®æ ¼å¼æ­£ç¡®
    if not isinstance(data, dict) or 'response' not in data:
        print(data)
        raise ValueError("æ•°æ®æ ¼å¼ä¸æ­£ç¡®ï¼Œç¼ºå°‘'response'å­—æ®µ")
    
    # æå–æ‰€æœ‰valueå€¼
    values = []
    for item in data['response']:
        if isinstance(item, dict) and 'value' in item:
            values.append(item['value'])
    return values


def analyze_data(A, B, C,x):
    """
    è®¡ç®—æ•°æ®çš„æ—¥ç¯æ¯”ã€çŸ­æœŸç¯æ¯”ã€å‡å€¼å¹¶åˆ¤æ–­æ˜¯å¦è¶…è¿‡é˜ˆå€¼ã€‚
    ç¯æ¯”ç»“æœæ ¼å¼åŒ–ä¸ºç®­å¤´+ç¬¦å·+ç™¾åˆ†æ¯”çš„å­—ç¬¦ä¸²ã€‚
    ä¸Šæ¶¨ï¼šâ†‘+xx.xx%
    ä¸‹é™ï¼šâ†“-xx.xx%
    
    å‚æ•°:
    A (list): å½“å‰æ—¶æ®µæ•°æ®åˆ—è¡¨
    B (list): ä¸Šä¸€ä¸ªå‘¨æœŸæ•°æ®åˆ—è¡¨ (ç”¨äºæ—¥ç¯æ¯”)
    C (list): çŸ­æœŸå†å²æ•°æ®åˆ—è¡¨ (ç”¨äºçŸ­æœŸç¯æ¯”)
    x (int): é˜ˆå€¼
    
    è¿”å›:
    dict: åŒ…å«è®¡ç®—ç»“æœçš„å­—å…¸
    """
    
    # è®¡ç®—å‡å€¼ï¼Œå¤„ç†åˆ—è¡¨ä¸ºç©ºçš„æƒ…å†µä»¥é˜²é™¤é›¶é”™è¯¯
    avg_A = sum(A) / len(A) if A else 0
    avg_B = sum(B) / len(B) if B else 0
    avg_C = sum(C) / len(C) if C else 0
    
    # è®¡ç®—æ—¥ç¯æ¯”æ•°å€¼
    try:
        day_ratio_val = (avg_A - avg_B) / avg_B
    except ZeroDivisionError:
        day_ratio_val = 0
 
    # è®¡ç®—çŸ­æœŸç¯æ¯”æ•°å€¼
    try:
        short_ratio_val = (avg_A - avg_C) / avg_C
    except ZeroDivisionError:
        short_ratio_val = 0
 
    # æ ¼å¼åŒ–é€»è¾‘ï¼šä¸Šæ¶¨ç”¨â†‘å’Œ+ï¼Œä¸‹é™ç”¨â†“å’Œ-
    # è¿™é‡Œçš„ç¬¦å·ä¸æ•°å€¼æœ¬èº«çš„æ­£è´Ÿå·ä¸€è‡´
    
    # æ—¥ç¯æ¯”å­—ç¬¦ä¸²ç”Ÿæˆ
    if day_ratio_val >= 0:
        day_ratio_str = f"â†‘{day_ratio_val * 100:+.2f}%"
    else:
        day_ratio_str = f"â†“{day_ratio_val * 100:+.2f}%"
 
    # çŸ­æœŸç¯æ¯”å­—ç¬¦ä¸²ç”Ÿæˆ
    if short_ratio_val >= 0:
        short_ratio_str = f"â†‘{short_ratio_val * 100:+.2f}%"
    else:
        short_ratio_str = f"â†“{short_ratio_val * 100:+.2f}%"
 
    # å‡å€¼
    mean_val = int(avg_A)
 
    # åˆå§‹åŒ–å‘Šè­¦ä¿¡æ¯
    alert_messages = []
    
 # åˆ¤æ–­æ—¥ç¯æ¯”ç»å¯¹å€¼æ˜¯å¦ > 20%ï¼ˆä¿®æ”¹ä¸º20%ï¼‰
    if abs(day_ratio_val * 100) > 20:
        alert_messages.append(f"æ—¥ç¯æ¯”>{20}%")
        
    # åˆ¤æ–­çŸ­æœŸç¯æ¯”ç»å¯¹å€¼æ˜¯å¦ > 20%ï¼ˆä¿®æ”¹ä¸º20%ï¼‰
    if abs(short_ratio_val * 100) > 20:
        alert_messages.append(f"çŸ­æœŸç¯æ¯”>{20}%")
    
    # åˆ¤æ–­æœ€å¤§å€¼æ˜¯å¦è¶…è¿‡å›ºå®šé˜ˆå€¼
    max_A = max(A) if A else 0
    if max_A >= x:
        alert_messages.append(f"Max = {max_A} >{x} (é˜ˆå€¼)")
        # print(max_A)
    
    # ç»„åˆæœ€ç»ˆå‘Šè­¦æ–‡æœ¬
    if alert_messages:
        threshold_status = "å¼‚å¸¸ ," + ",".join(alert_messages)
    else:
        threshold_status = "æ­£å¸¸"
 
    return {
        "æ—¥ç¯æ¯”": day_ratio_str,
        "çŸ­æœŸç¯æ¯”": short_ratio_str,
        "å‡å€¼": mean_val,
        "æ˜¯å¦å‘Šè­¦": threshold_status
    }


def day_add_1(data):
    """
    å°†å­—å…¸ä¸­æ‰€æœ‰æ—¶é—´é”®æ•´ä½“å‘åæ¨ç§»1å¤©
    """
    result = {}
    for metric, time_dict in data.items():
        new_time_dict = {}
        for time_str, value in time_dict.items():
            # å°†å­—ç¬¦ä¸²æ—¶é—´è½¬æ¢ä¸ºdatetimeï¼ŒåŠ ä¸Š1å¤©ï¼Œå†è½¬å›å­—ç¬¦ä¸²
            dt = datetime.strptime(time_str, "%Y-%m-%d %H:%M:%S")
            new_dt = dt + timedelta(days=1)
            new_time_str = new_dt.strftime("%Y-%m-%d %H:%M:%S")
            new_time_dict[new_time_str] = value
        
        result[metric] = new_time_dict
    
    return result
def day_add_30(data, minutes=30):
    """
    å°†å­—å…¸ä¸­æ‰€æœ‰æ—¶é—´é”®æ•´ä½“å‘åæ¨ç§»æŒ‡å®šåˆ†é’Ÿæ•°
    
    Args:
        data: åŒ…å«æ—¶é—´æˆ³ä½œä¸ºé”®çš„åµŒå¥—å­—å…¸
        minutes: è¦æ¨ç§»çš„åˆ†é’Ÿæ•°ï¼ˆé»˜è®¤30åˆ†é’Ÿï¼‰
        
    Returns:
        æ›´æ–°åçš„å­—å…¸
    """
    return {
        metric: {
            (datetime.strptime(t, "%Y-%m-%d %H:%M:%S") + timedelta(minutes=minutes))
            .strftime("%Y-%m-%d %H:%M:%S"): v
            for t, v in time_dict.items()
        }
        for metric, time_dict in data.items()
    }
    
def merge_dicts(dict1, dict2, dict3):
    """
    åˆå¹¶ä¸¤ä¸ªç›‘æ§æŒ‡æ ‡å­—å…¸
    å¦‚æœé”®ç›¸åŒï¼Œç¬¬äºŒä¸ªå­—å…¸çš„å€¼ä¼šè¦†ç›–ç¬¬ä¸€ä¸ªå­—å…¸çš„å€¼
    """
    return {**dict1, **dict2, **dict3}

def extract_qps_timeSeriesData(response_data):
    """
    æå–å¹¶æ ¼å¼åŒ–æ¥å£è¿”å›çš„æ•°æ®
    
    Args:
        response_data: æ¥å£è¿”å›çš„åŸå§‹æ•°æ®ï¼ˆå­—å…¸æ ¼å¼ï¼‰
    
    Returns:
        dict: æ ¼å¼åŒ–åçš„æ•°æ®ï¼Œæ ¼å¼å¦‚sample_data
    """
    try:
        # è§£æå“åº”æ•°æ®
        data_list = response_data.get('response', [])
        if not data_list:
            return {"QPS": {}}
        
        # åˆ›å»ºç»“æœå­—å…¸
        result = {"QPS": {}}
        
        # å¤„ç†æ¯ä¸ªæ•°æ®ç‚¹
        for item in data_list:
            timestamp = item.get('time')
            value = item.get('value')
            
            if timestamp is not None and value is not None:
                # å°†æ¯«ç§’æ—¶é—´æˆ³è½¬æ¢ä¸ºå¯è¯»æ—¶é—´æ ¼å¼
                # æ³¨æ„ï¼šè¿™é‡Œå‡è®¾æ—¶é—´æˆ³æ˜¯æ¯«ç§’çº§
                dt = datetime.fromtimestamp(timestamp / 1000)
                formatted_time = dt.strftime("%Y-%m-%d %H:%M:%S")
                
                # å°†å€¼è½¬æ¢ä¸ºæµ®ç‚¹æ•°ï¼ˆå¯æ ¹æ®éœ€è¦è°ƒæ•´ï¼‰
                formatted_value = float(value)
                
                # æ·»åŠ åˆ°ç»“æœä¸­
                result["QPS"][formatted_time] = formatted_value
        
        return result
    
    except Exception as e:
        print(f"æ•°æ®å¤„ç†å‡ºé”™: {e}")
        return {"QPS": {}}


    # name="get_cluster_qps_analysis",
    # description="è·å–æŒ‡å®šé›†ç¾¤æ—¶é—´èŒƒå›´å†…çš„QPS(æµé‡)æŒ‡æ ‡åˆ†æç»“æœ",
    # args_schema={
    #     "type": "object",
    #     "properties": {
    #         "groupname": {"type": "string", "description": "é›†ç¾¤åç§°(ä¾‹å¦‚: ga-lan-jdns1)"},
    #         "begin_time": {
    #             "type": "string",
    #             "description": "å¼€å§‹æ—¶é—´(ä¾‹å¦‚: 2026-01-13 09:00:00),è¦æ±‚æ ¼å¼ YYYY-MM-DD HH:MM:SS ",
    #         },
    #         "end_time": {
    #             "type": "string",
    #             "description": "ç»“æŸæ—¶é—´(ä¾‹å¦‚: 2026-01-13 09:30:00)ï¼Œè¦æ±‚æ ¼å¼ YYYY-MM-DD HH:MM:SS ",
    #         },
    #     },
    #     "required": ["groupname", "begin_time", "end_time"],
    # }
def get_cluster_qps_analysis(groupname, begin_time, end_time):
    """
    è·å–ä¸‰ä¸ªæ—¶é—´æ®µçš„é›†ç¾¤QPSæ•°æ®
    """
    # è·å–æ˜¨å¤©æ—¶é—´çš„å‚æ•°
    B_groupname, B_begin_time, B_end_time = get_yesterday_time(groupname, begin_time, end_time)
    # è·å–å‰30åˆ†é’Ÿçš„å‚æ•°
    C_groupname, C_begin_time, C_end_time = get_previous_30_minutes(groupname, begin_time, end_time)
    
    # è°ƒç”¨APIå‡½æ•°è·å–åŸå§‹æ•°æ®
    A_raw = get_cluster_qps_api(groupname, begin_time, end_time)
    B_raw = get_cluster_qps_api(B_groupname, B_begin_time, B_end_time)
    C_raw = get_cluster_qps_api(C_groupname, C_begin_time, C_end_time)
    
    A_ts = extract_qps_timeSeriesData(A_raw)
    B_ts = extract_qps_timeSeriesData(B_raw)
    C_ts = extract_qps_timeSeriesData(C_raw)
    
    if "QPS" in A_ts:
        A_ts["ğŸ“Š å½“å‰è¶‹åŠ¿"] = A_ts.pop("QPS")
    if "QPS" in B_ts:
        B_ts["ğŸ“ˆ æ—¥ç¯æ¯”"] = B_ts.pop("QPS")
    if "QPS" in C_ts:
        C_ts["ğŸ“ˆ 30åˆ†é’Ÿç¯æ¯”"] = C_ts.pop("QPS")
    
    merge_data = merge_dicts(A_ts, day_add_1(B_ts), day_add_30(C_ts))

    image_url = generate_timeseries_chart_url(merge_data,"svg","QPS",1300000)

    
    # ä»APIè¿”å›æ•°æ®ä¸­æå–QPSå€¼åˆ—è¡¨
    A_qps_data = extract_values(A_raw)
    B_qps_data = extract_values(B_raw)
    C_qps_data = extract_values(C_raw)

    obj = {
        "info":analyze_data(A_qps_data, B_qps_data, C_qps_data,1300000),
        "image_url": image_url,
    }
    

    return obj

# r = get_cluster_qps_analysis("lf-lan-ha1","2026-02-05 08:00:00","2026-02-05 8:30:00")
# print(r)