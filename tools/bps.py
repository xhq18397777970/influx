import time
import requests
import logging
import hashlib
from datetime import datetime, timedelta
from typing import Dict
 
#net_bps_in å…¥å£å¸¦å®½ï¼ˆä¸‹è½½ã€æ•°æ®è¿›å…¥è®¾å¤‡ï¼‰
#net_bps_out å‡ºå£å¸¦å®½ï¼ˆä¸Šä¼ ã€æ•°æ®ä»è®¾å¤‡å‡ºå»ï¼‰

CONFIG_get_image = {
    'appCode': 'JC_PIDLB',
    'token': '9b78f9ab773774f5b2c4b627ff007152',
    'api_url': 'http://deeplog-ck-robot.jd.com/rest/api/convertDataIntoImages',
}

#é‰´æƒ
def get_auth_headers_get_image() -> dict:
    """ç”Ÿæˆé‰´æƒè¯·æ±‚å¤´"""
    now = datetime.now()
    time_str = now.strftime("%H%M%Y%m%d")
    timestamp = str(int(time.time()))
    sign = hashlib.md5(f"#{CONFIG_get_image['token']}NP{time_str}".encode()).hexdigest()
    return {
        "Content-Type": "application/json",
        "appCode": CONFIG_get_image['appCode'],
        "sign": sign,
        "time": timestamp,
    }

def generate_timeseries_chart_url(
    data: Dict[str, Dict[str, float]],
    chart_type: str = "svg" ,
    metrics_name: str = None,
    base_line:float = None
) -> str:
    """
    ç”Ÿæˆæ—¶åºå›¾å¹¶è¿”å›åœ¨çº¿é¢„è§ˆé“¾æ¥
    Args:
        data: æ—¶åºæ•°æ® {æŒ‡æ ‡å: {æ—¶é—´ç‚¹: æ•°å€¼}}     
    Returns:
        åœ¨çº¿é¢„è§ˆé“¾æ¥
    """
    # æ ¹æ®chart_typeè®¾ç½®filename
    if chart_type == "svg":
        filename = f"{metrics_name}.svg"
    # elif chart_type == "png":
    #     filename = "chart.png"
    # else:
    #     filename = "chart.png"  # é»˜è®¤
    
    params = {
        "timeSeriesData": data,
        "filename": filename,  # åŒ…å«åç¼€çš„æ–‡ä»¶å
        "title": metrics_name,
        "width":1500,
        "height":700,
        "ossType" : 1,
        "showLegend":True,
        "usingBaseLine":True,
        "baseLine":base_line,
        "baseLineName":"é˜ˆå€¼"
    }
    resp = requests.post(
        CONFIG_get_image['api_url'],
        headers=get_auth_headers_get_image(),
        json=params,
        timeout=300
    )
    result = resp.json()
    
    # print("ç”Ÿæˆå›¾ç‰‡çš„ä¿¡æ¯ï¼š\n")
    # print(result)
    if result.get("code") == 200:
        return result["data"]["src"]
    else:
        raise Exception(f"å›¾è¡¨ç”Ÿæˆå¤±è´¥: {result.get('msg', 'æœªçŸ¥é”™è¯¯')}")


def get_yesterday_time(cluster_name, start_time, end_time):
    def shift(time_str):
        dt = datetime.strptime(time_str, "%Y-%m-%d %H:%M:%S")
        return (dt - timedelta(days=1)).strftime("%Y-%m-%d %H:%M:%S")

    return cluster_name, shift(start_time), shift(end_time)


def get_previous_30_minutes(device_id, time1, time2):
    def shift(time_str):
        dt = datetime.strptime(time_str, "%Y-%m-%d %H:%M:%S")
        return (dt - timedelta(minutes=30)).strftime("%Y-%m-%d %H:%M:%S")
    
    return device_id, shift(time1), shift(time2)
#é‰´æƒ
def npa_summary_data(postdata, apiurl, method="POST"):
    user = "xiehanqi.jackson"
    ctime = str(int(time.time()))
    new_key = f"{user}|{ctime}"
    # ä¿®æ­£è¿™é‡Œï¼šä½¿ç”¨ hashlib.md5() æ¥è®¡ç®—å“ˆå¸Œå€¼
    api_header_val = f"{hashlib.md5(new_key.encode()).hexdigest()}|{ctime}"
    url = f'http://npa-test.jd.com{apiurl}'
    user_agent = 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)'
    headers = {'auth-api': api_header_val, 'auth-user': user, 'Content-Type': "application/json", 'User-Agent': user_agent}
    try:
        if method == "POST":
            response = requests.post(url, json=postdata, headers=headers)
        elif method == "GET":
            response = requests.get(url, params=postdata, headers=headers)
        else:
            return {"error": "ä¸æ”¯æŒçš„è¯·æ±‚æ–¹æ³•"}
            
        response.raise_for_status()
        result = response.json()
        logging.info(f"code:{response.status_code}, response:{response.text}")
        
        # æ£€æŸ¥å“åº”ç»“æœçš„ä¸‰ç§æƒ…å†µ
        if result.get('code') == 200:
            if result.get('data'):
                # æƒ…å†µ1: code=200ä¸”dataæ•°æ®å­˜åœ¨ä¸ä¸ºç©ºï¼ŒæˆåŠŸç›´æ¥è¿”å›æ¥å£æ•°æ®
                return result
            else:
                # æƒ…å†µ3: code=200ä½†dataä¸ºç©ºï¼Œè¿”å›æ¥å£æ•°æ®ï¼Œå¹¶æ–°å¢å±æ€§messageï¼š"æ¥å£æ•°æ®ä¸ºç©º"
                result['message'] = "å…¥å‚æ•°æ ¼å¼æœ‰è¯¯ã€æˆ–è¯¥æ—¶æ®µæ¥å£è¿”å›æ•°æ®ä¸ºç©º"
                return result
        else:
            # æƒ…å†µ2: code!=200è¿”å›é”™è¯¯ä¿¡æ¯
            return {"error": f"APIè¯·æ±‚å¤±è´¥ï¼Œcode: {result.get('code')}, message: {result.get('message', 'æœªçŸ¥é”™è¯¯')}"}
            
    except requests.RequestException as e:
        logging.error(f"API request error: {e}")
        return {"error": f"APIè¯·æ±‚å¼‚å¸¸: {str(e)}"}
    except ValueError as e:
        logging.error(f"JSONè§£æé”™è¯¯: {e}")
        return {"error": "å“åº”æ•°æ®æ ¼å¼é”™è¯¯"}
 
#è·å–æ—¶é—´æ®µï¼Œé›†ç¾¤ç½‘ç»œæŒ‡æ ‡
def get_network_api(groupname, begin_time, end_time):
    postdata = {"groupname":groupname,"begin_time":begin_time,"end_time":end_time}
    apiurl = "/prod-api/api/v2/analysis/prometheus/core?format=json"
    result = npa_summary_data(postdata, apiurl)
    return result
 
def extract_network_data(data):
    """ä»åŸå§‹æ•°æ®ä¸­æå–ç½‘ç»œè¾“å…¥å’Œè¾“å‡ºæ•°æ®åˆ—è¡¨
    """
    net_in_bps_max = []
    net_out_bps_max = []
    
    for item in data.get('data', []):
        if item.get('title') == 'ç½‘ç»œæŒ‡æ ‡':
            for series in item.get('series_data', []):
                if series.get('name') == 'net_in_bps_max':
                    net_in_bps_max = series.get('value', [])
                elif series.get('name') == 'net_out_bps_max':
                    net_out_bps_max = series.get('value', [])
            break  # æ‰¾åˆ°ç½‘ç»œæŒ‡æ ‡æ•°æ®åå°±åœæ­¢éå†
    
    return net_in_bps_max, net_out_bps_max

def analyze_data(A, B, C,x):
    """
    è®¡ç®—æ•°æ®çš„æ—¥ç¯æ¯”ã€çŸ­æœŸç¯æ¯”ã€å‡å€¼å¹¶åˆ¤æ–­æ˜¯å¦è¶…è¿‡é˜ˆå€¼ã€‚
    ç¯æ¯”ç»“æœæ ¼å¼åŒ–ä¸ºç®­å¤´+ç¬¦å·+ç™¾åˆ†æ¯”çš„å­—ç¬¦ä¸²ã€‚
    ä¸Šæ¶¨ï¼šâ†‘+xx.xx%
    ä¸‹é™ï¼šâ†“-xx.xx%
    
    å‚æ•°:
    A (list): å½“å‰æ—¶æ®µæ•°æ®åˆ—è¡¨
    B (list): ä¸Šä¸€ä¸ªå‘¨æœŸæ•°æ®åˆ—è¡¨ (ç”¨äºæ—¥ç¯æ¯”)
    C (list): çŸ­æœŸå†å²æ•°æ®åˆ—è¡¨ (ç”¨äºçŸ­æœŸç¯æ¯”)
    x (int): é˜ˆå€¼
    
    è¿”å›:
    dict: åŒ…å«è®¡ç®—ç»“æœçš„å­—å…¸
    """
    
    # è®¡ç®—å‡å€¼ï¼Œå¤„ç†åˆ—è¡¨ä¸ºç©ºçš„æƒ…å†µä»¥é˜²é™¤é›¶é”™è¯¯
    avg_A = sum(A) / len(A) if A else 0
    avg_B = sum(B) / len(B) if B else 0
    avg_C = sum(C) / len(C) if C else 0
    
    # è®¡ç®—æ—¥ç¯æ¯”æ•°å€¼
    try:
        day_ratio_val = (avg_A - avg_B) / avg_B
    except ZeroDivisionError:
        day_ratio_val = 0
 
    # è®¡ç®—çŸ­æœŸç¯æ¯”æ•°å€¼
    try:
        short_ratio_val = (avg_A - avg_C) / avg_C
    except ZeroDivisionError:
        short_ratio_val = 0
 
    # æ ¼å¼åŒ–é€»è¾‘ï¼šä¸Šæ¶¨ç”¨â†‘å’Œ+ï¼Œä¸‹é™ç”¨â†“å’Œ-
    # è¿™é‡Œçš„ç¬¦å·ä¸æ•°å€¼æœ¬èº«çš„æ­£è´Ÿå·ä¸€è‡´
    
    # æ—¥ç¯æ¯”å­—ç¬¦ä¸²ç”Ÿæˆ
    if day_ratio_val >= 0:
        day_ratio_str = f"â†‘{day_ratio_val * 100:+.2f}%"
    else:
        day_ratio_str = f"â†“{day_ratio_val * 100:+.2f}%"
 
    # çŸ­æœŸç¯æ¯”å­—ç¬¦ä¸²ç”Ÿæˆ
    if short_ratio_val >= 0:
        short_ratio_str = f"â†‘{short_ratio_val * 100:+.2f}%"
    else:
        short_ratio_str = f"â†“{short_ratio_val * 100:+.2f}%"
 
    # å‡å€¼
    mean_val = int(avg_A)
 
    # åˆå§‹åŒ–å‘Šè­¦ä¿¡æ¯
    alert_messages = []
    
 # åˆ¤æ–­æ—¥ç¯æ¯”ç»å¯¹å€¼æ˜¯å¦ > 20%ï¼ˆä¿®æ”¹ä¸º20%ï¼‰
    if abs(day_ratio_val * 100) > 20:
        alert_messages.append(f"æ—¥ç¯æ¯”>{20}%")
        
    # åˆ¤æ–­çŸ­æœŸç¯æ¯”ç»å¯¹å€¼æ˜¯å¦ > 20%ï¼ˆä¿®æ”¹ä¸º20%ï¼‰
    if abs(short_ratio_val * 100) > 20:
        alert_messages.append(f"çŸ­æœŸç¯æ¯”>{20}%")
    
    # åˆ¤æ–­æœ€å¤§å€¼æ˜¯å¦è¶…è¿‡å›ºå®šé˜ˆå€¼
    max_A = max(A) if A else 0
    if max_A >= x:
        alert_messages.append(f"Max = {max_A} >{x} (é˜ˆå€¼)")
        # print(max_A)
    
    # ç»„åˆæœ€ç»ˆå‘Šè­¦æ–‡æœ¬
    if alert_messages:
        threshold_status = "å¼‚å¸¸ ," + ",".join(alert_messages)
    else:
        threshold_status = "æ­£å¸¸"
 
    return {
        "æ—¥ç¯æ¯”": day_ratio_str,
        "çŸ­æœŸç¯æ¯”": short_ratio_str,
        "å‡å€¼": mean_val,
        "æ˜¯å¦å‘Šè­¦": threshold_status
    }

def day_add_1(data):
    """
    å°†å­—å…¸ä¸­æ‰€æœ‰æ—¶é—´é”®æ•´ä½“å‘åæ¨ç§»1å¤©
    """
    result = {}
    for metric, time_dict in data.items():
        new_time_dict = {}
        for time_str, value in time_dict.items():
            # å°†å­—ç¬¦ä¸²æ—¶é—´è½¬æ¢ä¸ºdatetimeï¼ŒåŠ ä¸Š1å¤©ï¼Œå†è½¬å›å­—ç¬¦ä¸²
            dt = datetime.strptime(time_str, "%Y-%m-%d %H:%M:%S")
            new_dt = dt + timedelta(days=1)
            new_time_str = new_dt.strftime("%Y-%m-%d %H:%M:%S")
            new_time_dict[new_time_str] = value
        
        result[metric] = new_time_dict
    
    return result
def day_add_30(data, minutes=30):
    """
    å°†å­—å…¸ä¸­æ‰€æœ‰æ—¶é—´é”®æ•´ä½“å‘åæ¨ç§»æŒ‡å®šåˆ†é’Ÿæ•°
    
    Args:
        data: åŒ…å«æ—¶é—´æˆ³ä½œä¸ºé”®çš„åµŒå¥—å­—å…¸
        minutes: è¦æ¨ç§»çš„åˆ†é’Ÿæ•°ï¼ˆé»˜è®¤30åˆ†é’Ÿï¼‰
        
    Returns:
        æ›´æ–°åçš„å­—å…¸
    """
    return {
        metric: {
            (datetime.strptime(t, "%Y-%m-%d %H:%M:%S") + timedelta(minutes=minutes))
            .strftime("%Y-%m-%d %H:%M:%S"): v
            for t, v in time_dict.items()
        }
        for metric, time_dict in data.items()
    }
    
def merge_dicts(dict1, dict2, dict3):
    """
    åˆå¹¶ä¸¤ä¸ªç›‘æ§æŒ‡æ ‡å­—å…¸
    å¦‚æœé”®ç›¸åŒï¼Œç¬¬äºŒä¸ªå­—å…¸çš„å€¼ä¼šè¦†ç›–ç¬¬ä¸€ä¸ªå­—å…¸çš„å€¼
    """
    return {**dict1, **dict2, **dict3}

def extract_qps_timeSeriesData(response_data):
    """
    æå–å¹¶æ ¼å¼åŒ–æ¥å£è¿”å›çš„æ•°æ®
    
    Args:
        response_data: æ¥å£è¿”å›çš„åŸå§‹æ•°æ®ï¼ˆå­—å…¸æ ¼å¼ï¼‰
    
    Returns:
        dict: æ ¼å¼åŒ–åçš„æ•°æ®ï¼Œæ ¼å¼å¦‚sample_data
    """
    try:
        # è§£æå“åº”æ•°æ®
        data_list = response_data.get('response', [])
        if not data_list:
            return {"QPS": {}}
        
        # åˆ›å»ºç»“æœå­—å…¸
        result = {"QPS": {}}
        
        # å¤„ç†æ¯ä¸ªæ•°æ®ç‚¹
        for item in data_list:
            timestamp = item.get('time')
            value = item.get('value')
            
            if timestamp is not None and value is not None:
                # å°†æ¯«ç§’æ—¶é—´æˆ³è½¬æ¢ä¸ºå¯è¯»æ—¶é—´æ ¼å¼
                # æ³¨æ„ï¼šè¿™é‡Œå‡è®¾æ—¶é—´æˆ³æ˜¯æ¯«ç§’çº§
                dt = datetime.fromtimestamp(timestamp / 1000)
                formatted_time = dt.strftime("%Y-%m-%d %H:%M:%S")
                
                # å°†å€¼è½¬æ¢ä¸ºæµ®ç‚¹æ•°ï¼ˆå¯æ ¹æ®éœ€è¦è°ƒæ•´ï¼‰
                formatted_value = float(value)
                
                # æ·»åŠ åˆ°ç»“æœä¸­
                result["QPS"][formatted_time] = formatted_value
        
        return result
    
    except Exception as e:
        print(f"æ•°æ®å¤„ç†å‡ºé”™: {e}")
        return {"QPS": {}}
def extract_bps_timeSeriesData(response_data):
    result = {}
    for chart in response_data.get('data', []):
        if chart.get('title') == 'ç½‘ç»œæŒ‡æ ‡':
            for i, legend in enumerate(chart['legend_data']):
                if legend in ['net_in_bps_max', 'net_out_bps_max']:
                    result[legend] = dict(zip(
                        chart['x_data'], 
                        chart['series_data'][i]['value']
                    ))
    return result

#     name="get_cluster_network_analysis",
#     description="è·å–æŒ‡å®šé›†ç¾¤æ—¶é—´èŒƒå›´å†…çš„ç½‘ç»œï¼ˆå‡ºå…¥å£å¸¦å®½ï¼‰æŒ‡æ ‡åˆ†æç»“æœ",
#     args_schema={
#         "type": "object",
#         "properties": {
#             "groupname": {"type": "string", "description": "é›†ç¾¤åç§°(ä¾‹å¦‚: ga-lan-jdns1)"},
#             "begin_time": {
#                 "type": "string",
#                 "description": "å¼€å§‹æ—¶é—´(ä¾‹å¦‚: 2026-01-13 09:00:00),è¦æ±‚æ ¼å¼ YYYY-MM-DD HH:MM:SS ",
#             },
#             "end_time": {
#                 "type": "string",
#                 "description": "ç»“æŸæ—¶é—´(ä¾‹å¦‚: 2026-01-13 09:30:00)ï¼Œè¦æ±‚æ ¼å¼ YYYY-MM-DD HH:MM:SS ",
#             },
#         },
#         "required": ["groupname", "begin_time", "end_time"],
#     }

def get_cluster_network_analysis(groupname, begin_time, end_time):
    """
    è·å–ä¸‰ä¸ªæ—¶é—´æ®µçš„é›†ç¾¤ç½‘ç»œæ•°æ®
    """
    # è·å–æ˜¨å¤©æ—¶é—´çš„å‚æ•°
    B_groupname, B_begin_time, B_end_time = get_yesterday_time(groupname, begin_time, end_time)
    # è·å–å‰30åˆ†é’Ÿçš„å‚æ•°
    C_groupname, C_begin_time, C_end_time = get_previous_30_minutes(groupname, begin_time, end_time)
    
    # è°ƒç”¨APIå‡½æ•°è·å–åŸå§‹æ•°æ®
    A_raw = get_network_api(groupname, begin_time, end_time)
    B_raw = get_network_api(B_groupname, B_begin_time, B_end_time)
    C_raw = get_network_api(C_groupname, C_begin_time, C_end_time)
    
    A_ts = extract_bps_timeSeriesData(A_raw)
    B_ts = extract_bps_timeSeriesData(B_raw)
    C_ts = extract_bps_timeSeriesData(C_raw)
    
    # print(A_ts)
    
    if "net_in_bps_max" in A_ts:
        A_ts["ğŸ“Š å½“å‰è¶‹åŠ¿ (å…¥å£å¸¦å®½)"] = A_ts.pop("net_in_bps_max")
    if "net_out_bps_max" in A_ts:
        A_ts["ğŸ“Š å½“å‰è¶‹åŠ¿ (å‡ºå£å¸¦å®½)"] = A_ts.pop("net_out_bps_max")
        
    if "net_in_bps_max" in B_ts:
        B_ts["ğŸ“ˆ æ—¥ç¯æ¯” (å…¥å£å¸¦å®½)"] = B_ts.pop("net_in_bps_max")
    if "net_out_bps_max" in B_ts:
        B_ts["ğŸ“ˆ æ—¥ç¯æ¯” (å‡ºå£å¸¦å®½)"] = B_ts.pop("net_out_bps_max")
        
    if "net_out_bps_max" in C_ts:
        C_ts["ğŸ“ˆ 30åˆ†é’Ÿç¯æ¯” (å‡ºå£å¸¦å®½)"] = C_ts.pop("net_out_bps_max")
    if "net_in_bps_max" in C_ts:
        C_ts["ğŸ“ˆ 30åˆ†é’Ÿç¯æ¯” (å…¥å£å¸¦å®½)"] = C_ts.pop("net_in_bps_max")

        
    merge_data = merge_dicts(A_ts, day_add_1(B_ts), day_add_30(C_ts))
    
    image_url = generate_timeseries_chart_url(merge_data,"svg","ç½‘ç»œæŒ‡æ ‡",97960988447)

    
    # ä»APIè¿”å›æ•°æ®ä¸­æå–QPSå€¼åˆ—è¡¨
    A_in_bps_max, A_out_bps_max = extract_network_data(A_raw)

    B_in_bps_max, B_out_bps_max = extract_network_data(B_raw)
    C_in_bps_max, C_out_bps_max = extract_network_data(C_raw)
    
    # è°ƒç”¨analyze_dataå‡½æ•°è¿›è¡Œåˆ†æ
    in_bps_result = analyze_data(A_in_bps_max, B_in_bps_max, C_in_bps_max,97960988447)
    out_bps_result = analyze_data(A_out_bps_max, B_out_bps_max, C_out_bps_max,97960988447)

    
    obj = {
        "info":{
            "ç½‘ç»œå…¥å£å¸¦å®½":in_bps_result,
            "ç½‘ç»œå‡ºå£å¸¦å®½":out_bps_result,
        },
        "image_url":image_url,  
    }
    return obj

# r = get_cluster_network_analysis("lf-lan-ha1", "2026-01-29 09:30:00", "2026-01-29 10:00:00")
# print(r)