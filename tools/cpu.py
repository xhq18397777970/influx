import time
import requests
import logging
import hashlib
import json
import sys
import os
import uuid

from datetime import datetime, timedelta
import time
import requests
from datetime import datetime
import hashlib
from typing import Dict
import random

from concurrent.futures import ThreadPoolExecutor
def parallel_execute(tasks):
    """
    å¹¶è¡Œæ‰§è¡Œå¤šä¸ªä»»åŠ¡
    
    Args:
        tasks: ä»»åŠ¡åˆ—è¡¨ï¼Œæ¯ä¸ªä»»åŠ¡æ˜¯ä¸€ä¸ªå…ƒç»„ (func, *args)
    
    Returns:
        list: æŒ‰ä»»åŠ¡é¡ºåºè¿”å›çš„ç»“æœåˆ—è¡¨
    """
    with ThreadPoolExecutor(max_workers=len(tasks)) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        futures = [executor.submit(task[0], *task[1:]) for task in tasks]
        # æŒ‰é¡ºåºè·å–ç»“æœ
        return [future.result() for future in futures]

CONFIG = {
    'appCode': 'JC_PIDLB',
    'token': '9b78f9ab773774f5b2c4b627ff007152',
    'api_url': 'http://deeplog-ck-robot.jd.com/rest/api/convertDataIntoImages',
}


#é‰´æƒ
def get_auth_headers() -> dict:
    """ç”Ÿæˆé‰´æƒè¯·æ±‚å¤´"""
    now = datetime.now()
    time_str = now.strftime("%H%M%Y%m%d")
    timestamp = str(int(time.time()))
    sign = hashlib.md5(f"#{CONFIG['token']}NP{time_str}".encode()).hexdigest()
    return {
        "Content-Type": "application/json",
        "appCode": CONFIG['appCode'],
        "sign": sign,
        "time": timestamp,
    }

def generate_timeseries_chart_url(
    data: Dict[str, Dict[str, float]],
    chart_type: str = "svg" ,
    metrics_name: str = None,
    base_line:float = None
) -> str:
    """
    ç”Ÿæˆæ—¶åºå›¾å¹¶è¿”å›åœ¨çº¿é¢„è§ˆé“¾æ¥
    Args:
        data: æ—¶åºæ•°æ® {æŒ‡æ ‡å: {æ—¶é—´ç‚¹: æ•°å€¼}}     
    Returns:
        åœ¨çº¿é¢„è§ˆé“¾æ¥
    """
    # æ ¹æ®chart_typeè®¾ç½®filename
    if chart_type == "svg":
        filename = f"{metrics_name}_{uuid.uuid4().hex}.{chart_type}"
    # elif chart_type == "png":
    #     filename = "chart.png"
    # else:
    #     filename = "chart.png"  # é»˜è®¤
    
    
    params = {
        "timeSeriesData": data,
        "filename": filename,  # åŒ…å«åç¼€çš„æ–‡ä»¶å
        "title": metrics_name,
        "width":1500,
        "height":700,
        "ossType" : 1,
        "showLegend":True,
        "usingBaseLine":True,
        "baseLine":base_line,
        "baseLineName":"é˜ˆå€¼"
    }
    
    resp = requests.post(
        CONFIG['api_url'],
        headers=get_auth_headers(),
        json=params,
        timeout=300
    )
    result = resp.json()
    
    # print("ç”Ÿæˆå›¾ç‰‡çš„ä¿¡æ¯ï¼š\n")
    # print(result)
    if result.get("code") == 200:
        return result["data"]["src"]
    else:
        raise Exception(f"å›¾è¡¨ç”Ÿæˆå¤±è´¥: {result.get('msg', 'æœªçŸ¥é”™è¯¯')}")


from typing import Dict, List, Tuple, Any
from datetime import datetime, timedelta

def get_yesterday_time(cluster_name, start_time, end_time):
    def shift(time_str):
        dt = datetime.strptime(time_str, "%Y-%m-%d %H:%M:%S")
        return (dt - timedelta(days=1)).strftime("%Y-%m-%d %H:%M:%S")

    return cluster_name, shift(start_time), shift(end_time)


def get_previous_30_minutes(device_id, time1, time2):
    def shift(time_str):
        dt = datetime.strptime(time_str, "%Y-%m-%d %H:%M:%S")
        return (dt - timedelta(minutes=30)).strftime("%Y-%m-%d %H:%M:%S")
    
    return device_id, shift(time1), shift(time2)

# é‰´æƒ
def npa_summary_data(postdata, apiurl, method="POST"):
    user = "xiehanqi.jackson"
    ctime = str(int(time.time()))
    new_key = f"{user}|{ctime}"
    # è®¡ç®—å“ˆå¸Œå€¼
    api_header_val = f"{hashlib.md5(new_key.encode()).hexdigest()}|{ctime}"
    url = f'http://npa-test.jd.com{apiurl}'
    user_agent = 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)'
    headers = {
        'auth-api': api_header_val,
        'auth-user': user,
        'Content-Type': "application/json",
        'User-Agent': user_agent
    }
    try:
        if method == "POST":
            response = requests.post(url, json=postdata, headers=headers)
        elif method == "GET": # ä¿®æ­£ï¼šä½¿ç”¨ elif é¿å…é€»è¾‘æ¼æ´
            response = requests.get(url, params=postdata, headers=headers)
        else:
            raise ValueError(f"Unsupported HTTP method: {method}")
            
        response.raise_for_status() # æ£€æŸ¥ HTTP çŠ¶æ€ç  (4xx, 5xx ä¼šæŠ›å¼‚å¸¸)
        
        logging.info(f"code:{response.status_code}, response:{response.text}")
        return response.json()
    except requests.RequestException as e:
        logging.error(f"API request error: {e}")
        return None # å‘ç”Ÿç½‘ç»œæˆ–HTTPé”™è¯¯æ—¶è¿”å› Noneï¼Œä¾¿äºä¸Šå±‚åˆ¤æ–­


# è·å–CPUæ•°æ®,éœ€è¦å‚æ•°ï¼Œèµ·æ­¢æ—¶é—´ã€é›†ç¾¤åç§°
def get_cluster_cpu_api(groupname,begin_time,end_time):
    postdata = {
        "groupname": groupname,
        "begin_time": begin_time,
        "end_time": end_time
    }
    apiurl = "/prod-api/api/v2/analysis/prometheus/core?format=json"
    result = npa_summary_data(postdata, apiurl)
    # --- è¡¥å……é”™è¯¯å¤„ç† ---
    # 1. æ£€æŸ¥åŸºç¡€è¯·æ±‚æ˜¯å¦æˆåŠŸ (npa_summary_data å¯èƒ½è¿”å› None)
    if result is None:
        logging.error("Failed to fetch data due to request exception.")
        return {"status": "error", "message": "Request failed", "code": -1}
 
    # 2. æ£€æŸ¥ä¸šåŠ¡çŠ¶æ€ç  (å‡è®¾ API è¿”å›æ ¼å¼ä¸­åŒ…å« code å­—æ®µ)
    # å¾ˆå¤š API è¿”å›æ ¼å¼ä¸º {"code": 200, "data": ...} æˆ– {"code": 500, "msg": "error"}
    if "code" in result:
        business_code = result.get("code")
        # å‡è®¾ 200 è¡¨ç¤ºæˆåŠŸï¼Œå…¶ä»–å‡ä¸ºä¸šåŠ¡é”™è¯¯
        if business_code != 200: 
            error_msg = result.get("message", result.get("msg", "Unknown business error"))
            logging.error(f"API business error: code={business_code}, message={error_msg}")
            return {
                "status": "error", 
                "message": error_msg, 
                "code": business_code,
                "raw_response": result
            }
 
    # 3. æ£€æŸ¥æ•°æ®å†…å®¹æ˜¯å¦å­˜åœ¨ (é˜²æ­¢ code=200 ä½† data ä¸ºç©º)
    # å‡è®¾æœ‰æ•ˆæ•°æ®åœ¨ 'data' å­—æ®µä¸­
    if "data" not in result or not result["data"]:
        logging.warning("API response successful but no data found.")
        return {"status": "empty", "message": "No data available", "code": 0}
 
    # å…¨éƒ¨æ£€æŸ¥é€šè¿‡ï¼Œè¿”å›å®Œæ•´ç»“æœ
    return result


def analyze_data(A, B, C, D,x):
    """
    è®¡ç®—æ•°æ®çš„æ—¥ç¯æ¯”ã€çŸ­æœŸç¯æ¯”ã€å‡å€¼å¹¶åˆ¤æ–­æ˜¯å¦è¶…è¿‡é˜ˆå€¼ã€‚
    ç¯æ¯”ç»“æœæ ¼å¼åŒ–ä¸ºç®­å¤´+ç¬¦å·+ç™¾åˆ†æ¯”çš„å­—ç¬¦ä¸²ã€‚
    ä¸Šæ¶¨ï¼šâ†‘+xx.xx%
    ä¸‹é™ï¼šâ†“-xx.xx%
    
    å‚æ•°:
    A (list): å½“å‰æ—¶æ®µæ•°æ®åˆ—è¡¨
    B (list): ä¸Šä¸€ä¸ªå‘¨æœŸæ•°æ®åˆ—è¡¨ (ç”¨äºæ—¥ç¯æ¯”)
    C (list): çŸ­æœŸå†å²æ•°æ®åˆ—è¡¨ (ç”¨äºçŸ­æœŸç¯æ¯”)
    x (int): é˜ˆå€¼
    
    è¿”å›:
    dict: åŒ…å«è®¡ç®—ç»“æœçš„å­—å…¸
    """
    
    # è®¡ç®—å‡å€¼ï¼Œå¤„ç†åˆ—è¡¨ä¸ºç©ºçš„æƒ…å†µä»¥é˜²é™¤é›¶é”™è¯¯
    avg_A = sum(A) / len(A) if A else 0
    avg_B = sum(B) / len(B) if B else 0
    avg_C = sum(C) / len(C) if C else 0
    
    # è®¡ç®—æ—¥ç¯æ¯”æ•°å€¼
    try:
        day_ratio_val = (avg_A - avg_B) / avg_B
    except ZeroDivisionError:
        day_ratio_val = 0
 
    # è®¡ç®—çŸ­æœŸç¯æ¯”æ•°å€¼
    try:
        short_ratio_val = (avg_A - avg_C) / avg_C
    except ZeroDivisionError:
        short_ratio_val = 0
 
    # æ ¼å¼åŒ–é€»è¾‘ï¼šä¸Šæ¶¨ç”¨â†‘å’Œ+ï¼Œä¸‹é™ç”¨â†“å’Œ-
    # è¿™é‡Œçš„ç¬¦å·ä¸æ•°å€¼æœ¬èº«çš„æ­£è´Ÿå·ä¸€è‡´
    
    # æ—¥ç¯æ¯”å­—ç¬¦ä¸²ç”Ÿæˆ
    if day_ratio_val >= 0:
        day_ratio_str = f"â†‘{day_ratio_val * 100:+.2f}%"
    else:
        day_ratio_str = f"â†“{day_ratio_val * 100:+.2f}%"
 
    # çŸ­æœŸç¯æ¯”å­—ç¬¦ä¸²ç”Ÿæˆ
    if short_ratio_val >= 0:
        short_ratio_str = f"â†‘{short_ratio_val * 100:+.2f}%"
    else:
        short_ratio_str = f"â†“{short_ratio_val * 100:+.2f}%"
 
    # å‡å€¼
    mean_val = int(avg_A)
 
    # åˆå§‹åŒ–å‘Šè­¦ä¿¡æ¯
    alert_messages = []
    
    # åˆ¤æ–­æ—¥ç¯æ¯”æ˜¯å¦ > 30%
    if day_ratio_val * 100 > 30:
        alert_messages.append(f"æ—¥ç¯æ¯”>30%")
        
    # åˆ¤æ–­çŸ­æœŸç¯æ¯”æ˜¯å¦ > 30%
    if short_ratio_val * 100 > 30:
        alert_messages.append(f"çŸ­æœŸç¯æ¯”>30%")
    
    # åˆ¤æ–­æœ€å¤§å€¼æ˜¯å¦è¶…è¿‡å›ºå®šé˜ˆå€¼
    max_D = max(D) if D else 0
    if max_D >= x:
        alert_messages.append(f"Max = {max_D} >{x} (é˜ˆå€¼)")
        print(max_D)
    
    # ç»„åˆæœ€ç»ˆå‘Šè­¦æ–‡æœ¬
    if alert_messages:
        threshold_status = "å¼‚å¸¸ ," + ",".join(alert_messages)
    else:
        threshold_status = "æ­£å¸¸"
 
    return {
        "æ—¥ç¯æ¯”": day_ratio_str,
        "çŸ­æœŸç¯æ¯”": short_ratio_str,
        "å‡å€¼": mean_val,
        "æ˜¯å¦å‘Šè­¦": threshold_status
    }

def extract_cpu_max(data):
    """
    ä»æ¥å£è¿”å›çš„æ•°æ®ä¸­æå–CPUå¹³å‡å€¼åˆ—è¡¨
    
    å‚æ•°:
        data: æ¥å£è¿”å›çš„å®Œæ•´æ•°æ®å­—å…¸
    
    è¿”å›:
        list: CPUå¹³å‡å€¼åˆ—è¡¨ï¼Œå¦‚æœæ‰¾ä¸åˆ°åˆ™è¿”å›ç©ºåˆ—è¡¨
    """
    if not isinstance(data, dict):
        return []
    
    # æ£€æŸ¥å“åº”ç 
    if data.get('code') != 200:
        print(f"é”™è¯¯å“åº”ç : {data.get('code')}")
        return []
    
    # è·å–dataå­—æ®µ
    data_list = data.get('data', [])
    
    # æŸ¥æ‰¾CPUæŒ‡æ ‡
    for item in data_list:
        if isinstance(item, dict) and item.get('title') == 'CPUæŒ‡æ ‡':
            # æ‰¾åˆ°series_data
            series_data = item.get('series_data', [])
            
            # æŸ¥æ‰¾cluster_cpu_avgæ•°æ®
            for series in series_data:
                if series.get('name') == 'cluster_cpu_max':
                    return series.get('value', [])
    
    print("æœªæ‰¾åˆ°CPU max æ•°æ®")
    return []
def extract_cpu_avg(data):
    """
    ä»æ¥å£è¿”å›çš„æ•°æ®ä¸­æå–CPUå¹³å‡å€¼åˆ—è¡¨
    
    å‚æ•°:
        data: æ¥å£è¿”å›çš„å®Œæ•´æ•°æ®å­—å…¸
    
    è¿”å›:
        list: CPUå¹³å‡å€¼åˆ—è¡¨ï¼Œå¦‚æœæ‰¾ä¸åˆ°åˆ™è¿”å›ç©ºåˆ—è¡¨
    """
    if not isinstance(data, dict):
        return []
    
    # æ£€æŸ¥å“åº”ç 
    if data.get('code') != 200:
        print(f"é”™è¯¯å“åº”ç : {data.get('code')}")
        return []
    
    # è·å–dataå­—æ®µ
    data_list = data.get('data', [])
    
    # æŸ¥æ‰¾CPUæŒ‡æ ‡
    for item in data_list:
        if isinstance(item, dict) and item.get('title') == 'CPUæŒ‡æ ‡':
            # æ‰¾åˆ°series_data
            series_data = item.get('series_data', [])
            
            # æŸ¥æ‰¾cluster_cpu_avgæ•°æ®
            for series in series_data:
                if series.get('name') == 'cluster_cpu_avg':
                    return series.get('value', [])
    
    print("æœªæ‰¾åˆ°CPUå¹³å‡å€¼æ•°æ®")
    return []
def day_add_1(data):
    """
    å°†å­—å…¸ä¸­æ‰€æœ‰æ—¶é—´é”®æ•´ä½“å‘åæ¨ç§»1å¤©
    """
    result = {}
    for metric, time_dict in data.items():
        new_time_dict = {}
        for time_str, value in time_dict.items():
            # å°†å­—ç¬¦ä¸²æ—¶é—´è½¬æ¢ä¸ºdatetimeï¼ŒåŠ ä¸Š1å¤©ï¼Œå†è½¬å›å­—ç¬¦ä¸²
            dt = datetime.strptime(time_str, "%Y-%m-%d %H:%M:%S")
            new_dt = dt + timedelta(days=1)
            new_time_str = new_dt.strftime("%Y-%m-%d %H:%M:%S")
            new_time_dict[new_time_str] = value
        
        result[metric] = new_time_dict
    
    return result
def day_add_30(data, minutes=30):
    """
    å°†å­—å…¸ä¸­æ‰€æœ‰æ—¶é—´é”®æ•´ä½“å‘åæ¨ç§»æŒ‡å®šåˆ†é’Ÿæ•°
    
    Args:
        data: åŒ…å«æ—¶é—´æˆ³ä½œä¸ºé”®çš„åµŒå¥—å­—å…¸
        minutes: è¦æ¨ç§»çš„åˆ†é’Ÿæ•°ï¼ˆé»˜è®¤30åˆ†é’Ÿï¼‰
        
    Returns:
        æ›´æ–°åçš„å­—å…¸
    """
    return {
        metric: {
            (datetime.strptime(t, "%Y-%m-%d %H:%M:%S") + timedelta(minutes=minutes))
            .strftime("%Y-%m-%d %H:%M:%S"): v
            for t, v in time_dict.items()
        }
        for metric, time_dict in data.items()
    }
    
def merge_dicts(dict1, dict2, dict3):
    """
    åˆå¹¶ä¸¤ä¸ªç›‘æ§æŒ‡æ ‡å­—å…¸
    å¦‚æœé”®ç›¸åŒï¼Œç¬¬äºŒä¸ªå­—å…¸çš„å€¼ä¼šè¦†ç›–ç¬¬ä¸€ä¸ªå­—å…¸çš„å€¼
    """
    return {**dict1, **dict2, **dict3}

def extract_cpu_timeSeriesData(api_response):
    """æå–CPUæŒ‡æ ‡æ•°æ®å¹¶è½¬æ¢ä¸ºæŒ‡å®šæ ¼å¼"""
    if isinstance(api_response, str):
        data = json.loads(api_response)
    else:
        data = api_response
    
    # è·å–CPUæŒ‡æ ‡éƒ¨åˆ†
    cpu_data = None
    for item in data.get("data", []):
        if item.get("title") == "CPUæŒ‡æ ‡":
            cpu_data = item
            break
    
    if not cpu_data:
        return {}
    
    # æ„å»ºç»“æœå­—å…¸
    result = {
        "CPUä½¿ç”¨ç‡": {},
        "CPUæœ€å¤§å€¼": {}
    }
    
    x_data = cpu_data.get("x_data", [])
    series = cpu_data.get("series_data", [])
    
    # éå†æ¯æ¡æ•°æ®ç³»åˆ—
    for series_item in series:
        name = series_item.get("name")
        values = series_item.get("value", [])
        
        # æ˜ å°„æŒ‡æ ‡åç§°
        if name == "cluster_cpu_avg":
            key = "CPUä½¿ç”¨ç‡"

        elif name == "cluster_cpu_max":
            key = "CPUæœ€å¤§å€¼"
        else:
            continue
        
        # æ·»åŠ æ—¶é—´-å€¼å¯¹
        for time, value in zip(x_data, values):
            result[key][time] = value
    
    return result


    
from app.tools.registry import tool
# è·å–é›†ç¾¤æŒ‡å®šæ—¶é—´æ®µçš„CPUåˆ†æç»“æœ
@tool(
    name="get_cluster_cpu_analysis",
    description="è·å–æŒ‡å®šé›†ç¾¤åœ¨æ—¶é—´èŒƒå›´å†…çš„CPUæŒ‡æ ‡åˆ†æç»“æœ",
    args_schema={
        "type": "object",
        "properties": {
            "groupname": {"type": "string", "description": "é›†ç¾¤åç§°(ä¾‹å¦‚: lf-lan-ha1)"},
            "begin_time": {
                "type": "string",
                "description": "å¼€å§‹æ—¶é—´(è¦æ±‚æ ¼å¼ YYYY-MM-DD HH:MM:SS)",
            },
            "end_time": {
                "type": "string",
                "description": "ç»“æŸæ—¶é—´(è¦æ±‚æ ¼å¼ YYYY-MM-DD HH:MM:SS)",
            },
        },
        "required": ["groupname", "begin_time", "end_time"],
    },
)
def get_cluster_cpu_analysis(groupname, begin_time, end_time):
    # è·å–æ˜¨å¤©æ—¶é—´çš„å‚æ•°
    B_groupname, B_begin_time, B_end_time = get_yesterday_time(groupname, begin_time, end_time)
    # è·å–å‰30åˆ†é’Ÿçš„å‚æ•°
    C_groupname, C_begin_time, C_end_time = get_previous_30_minutes(groupname, begin_time, end_time)
    
    # å¹¶è¡Œè°ƒç”¨APIå‡½æ•°
    tasks = [
        (get_cluster_cpu_api, groupname, begin_time, end_time),
        (get_cluster_cpu_api, B_groupname, B_begin_time, B_end_time),
        (get_cluster_cpu_api, C_groupname, C_begin_time, C_end_time)
    ]
    A_list, B_list, C_list = parallel_execute(tasks)
    
    A_ts = extract_cpu_timeSeriesData(A_list)
    B_ts = extract_cpu_timeSeriesData(B_list)
    if "CPUä½¿ç”¨ç‡" in A_ts:
        A_ts["ğŸ“Š å½“å‰è¶‹åŠ¿ (å¹³å‡å€¼)"] = A_ts.pop("CPUä½¿ç”¨ç‡")
    if "CPUæœ€å¤§å€¼" in A_ts:
        A_ts["ğŸ“Š å½“å‰è¶‹åŠ¿ (å³°å€¼)"] = A_ts.pop("CPUæœ€å¤§å€¼")
        
    if "CPUä½¿ç”¨ç‡" in B_ts:
        B_ts["ğŸ“ˆ æ—¥ç¯æ¯” (å¹³å‡å€¼)"] = B_ts.pop("CPUä½¿ç”¨ç‡")
    if "CPUæœ€å¤§å€¼" in B_ts:
        B_ts["ğŸ“ˆ æ—¥ç¯æ¯” (å³°å€¼)"] = B_ts.pop("CPUæœ€å¤§å€¼")

    C_ts = extract_cpu_timeSeriesData(C_list)
    if "CPUä½¿ç”¨ç‡" in C_ts:
        C_ts["ğŸ“ˆ 30åˆ†é’Ÿç¯æ¯” (å¹³å‡å€¼)"] = C_ts.pop("CPUä½¿ç”¨ç‡")
    if "CPUæœ€å¤§å€¼" in C_ts:
        C_ts["ğŸ“ˆ 30åˆ†é’Ÿç¯æ¯” (å³°å€¼)"] = C_ts.pop("CPUæœ€å¤§å€¼")
    
    
    merged_data = merge_dicts(A_ts, day_add_1(B_ts), day_add_30(C_ts))
    
    # print(merged_data)
    
    image_url = generate_timeseries_chart_url(merged_data,"svg","CPUæŒ‡æ ‡",60)
    

    # ä»APIè¿”å›æ•°æ®ä¸­æå–CPUå¹³å‡å€¼åˆ—è¡¨
    tasks_extract = [
        (extract_cpu_avg,A_list),
        (extract_cpu_avg,B_list),
        (extract_cpu_avg,C_list)
    ]
    A_cpu_data, B_cpu_data, C_cpu_data = parallel_execute(tasks_extract)
    
    D_cpu_max_data = extract_cpu_max(A_list)
    
    obj = {
        "info":analyze_data(A_cpu_data, B_cpu_data, C_cpu_data,D_cpu_max_data, 60),
        "image_url": image_url,
        
    }
    
    # ä½¿ç”¨æå–çš„CPUæ•°æ®è¿›è¡Œåˆ†æ
    return obj


# import time

# start = time.time()
# print(get_cluster_cpu_analysis("lf-lan-ha1", "2026-02-05 11:00:00", "2026-02-05 11:30:00"))
# print(f"è€—æ—¶: {time.time()-start:.2f}s")